{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The File for generating HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wand.image import Image as wi\n",
    "import os; from io import BytesIO; import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from tesserocr import PyTessBaseAPI, RIL, iterate_level, PT, OEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../datafiles/dfile.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytestapi_path = \"C:\\\\Tesseract\\\\Tesseract-OCR-v4\\\\tessdata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf to Images List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2ImagesList(filepath):\n",
    "    pdf = wi(filename = filepath, resolution=800, background = 'white')\n",
    "    numPages = len(pdf.sequence)   #should be a class prop\n",
    "    pdfImages = pdf.convert(\"jpeg\")\n",
    "    \n",
    "    pageImages = []   #should be a class prop\n",
    "    for img in pdfImages.sequence:\n",
    "        page = wi(img)\n",
    "        PIL_img = Image.open(BytesIO(page.make_blob('jpeg')))\n",
    "        pageImages.append(PIL_img)\n",
    "    \n",
    "    return pageImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepageImages = pdf2ImagesList(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filepageImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise calculation [per page]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_val_Image(imagePIL, NoiseThreshold_ppage): # NoiseThreshold_ppage is b/w 0 and 1\n",
    "    '''\n",
    "    Params:\n",
    "        imagePIL - PIL image object\n",
    "        NoiseThreshold_ppage - %age noise threshold for an image range [0, 1]\n",
    "        \n",
    "    Returns:\n",
    "        ratio of dark pixels and total pixels\n",
    "        boolean page acceptance result based on noise ratio\n",
    "        binary image with 0 representing noise pixels\n",
    "    '''\n",
    "    \n",
    "    BinaryThreshold = 195  ## threshold per pixel \n",
    "    \n",
    "    grayPIL = imagePIL.convert('L')\n",
    "    imageNUMPY = np.array(grayPIL, dtype='uint8')\n",
    "    \n",
    "    Tessstart = time.time()\n",
    "    \n",
    "    with PyTessBaseAPI(path = \"C:\\\\Tesseract\\\\Tesseract-OCR-v3\\\\tessdata\") as api:\n",
    "        api.SetImage(imagePIL)\n",
    "        api.Recognize()\n",
    "        ri = api.GetIterator()\n",
    "        level = RIL.TEXTLINE\n",
    "        for r in iterate_level(ri, level):\n",
    "            x1, y1, x2, y2 = r.BoundingBox(level)\n",
    "            imageNUMPY[y1:y2, x1:x2] = 255\n",
    "    \n",
    "    bin_ = np.zeros(imageNUMPY.shape)\n",
    "    bin_ = np.where(imageNUMPY > BinaryThreshold, 1, 0)\n",
    "    \n",
    "    pixels_num = bin_.shape[0]*bin_.shape[1]\n",
    "    ratio = (pixels_num - bin_.sum())/pixels_num\n",
    "    \n",
    "    return ratio, ratio > NoiseThreshold_ppage, bin_ #returns the ratio, rejection result, binary_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_params_PDF(PDFImages, NoiseThreshold_ppage  = 0.50):\n",
    "    ''' IN-PROGRESS\n",
    "    PDFImages: r x c x ch x num where num is number of \n",
    "    '''\n",
    "    PAGESCOUNT = len(PDFImages)\n",
    "    pageNoiseFlag = [0]*PAGESCOUNT   #should be a class prop\n",
    "\n",
    "    FORstart = time.time()\n",
    "    \n",
    "    for i in range(0, len(PDFImages)):\n",
    "        #ITstart = time.time()\n",
    "        if(get_noise_val_Image(PDFImages[i], NoiseThreshold_ppage)[1]): #rejection means: page is noisy\n",
    "            pageNoiseFlag[i] = 1\n",
    "        #print(\"Iteration-Time taken\", time.time() - ITstart)\n",
    "    \n",
    "    print(\"FOR EndTime taken\", time.time() - FORstart)\n",
    "    \n",
    "    return   sum(pageNoiseFlag)/PAGESCOUNT, pageNoiseFlag #returns the %age of pages that are noisy [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EndTime taken 17.110139846801758\n"
     ]
    }
   ],
   "source": [
    "noiseratio, noise_bin = get_noise_params_PDF(filepageImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio:\t 0.0 \t\tList:\t [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ratio:\\t\", noiseratio, \"\\t\\tList:\\t\", noise_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR SCAN\n",
    "* Images to Text Conversion \n",
    "* Table Detection (Table detection does happens in the java code but doesn't go in the grain)\n",
    "     * TH , TR , TD\n",
    "* Background Detection\n",
    "    * Specific Line Background\n",
    "    * Word Level, Block Level & Line Level (Under Progress)\n",
    "    \n",
    "* Font Characterstics\n",
    "    * Font Density ( Tried to do in jar but was inefficent, insufficent and inaccurate)\n",
    "        * Bold \n",
    "        * Light\n",
    "    * Font Size\n",
    "* Hand Written Detection (ITS UNDER PROGRESS)\n",
    "    * HandWritten Model vs Digital Text Model\n",
    "    * Evaluation of Metrices\n",
    "    \n",
    "     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_bg_detection(bbox, numBIN, NUMPYimage):\n",
    "    if bbox:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        colorbin = np.ones((numBIN, 3), dtype = 'int16') *-1\n",
    "\n",
    "        combs = np.where(NUMPYimage[y1:y2, x1:x2, 0] != -1) # Check if the image didn't had the -1 there\n",
    "        if (combs[0].size != 0): # check if the size is not zero \n",
    "            bin_i = 0\n",
    "            # Choose randomly bins (20 is the default set right now )\n",
    "            # To check the image or the bin or whatsoever \n",
    "            for i in np.random.choice(range(combs[0].shape[0]), numBIN):\n",
    "                x = combs[1][i]\n",
    "                y = combs[0][i]\n",
    "                colorbin[bin_i] = NUMPYimage[y1+y, x1+x, :]\n",
    "                bin_i += 1\n",
    "            # Historgram \n",
    "\n",
    "            hist = {}\n",
    "            for r, g, b in colorbin:\n",
    "                if not (r == -1):\n",
    "                    RGB = str(r)+\"_\"+str(g)+\"_\"+str(b)\n",
    "                    if RGB in list(hist.keys()):\n",
    "                        hist[RGB] += 1\n",
    "                    else:\n",
    "                        hist[RGB] = 1\n",
    "            if len(list(hist.keys())):\n",
    "                return list(hist.keys())[list(hist.values()).index(max(list(hist.values())))].split(\"_\") ## RGB value in list\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "FileTextBuffer = \"\"\n",
    "FileTextBuffer += \"<document>\\n\"\n",
    "\n",
    "pagecounter  = 1\n",
    "for page in filepageImages[2:3]:\n",
    "    print(pagecounter)\n",
    "    thisPageText = \"\"\n",
    "    thisPageText += \"<page\" +str(pagecounter)+\">\\n\"\n",
    "    \n",
    "    with PyTessBaseAPI(path = pytestapi_path) as api:\n",
    "        api.SetImage(page)\n",
    "        api.Recognize()\n",
    "        \n",
    "        if(page.mode != 'RGB'):\n",
    "            newPIL = page.convert('RGB')\n",
    "        else:\n",
    "            newPIL = page.copy()\n",
    "            \n",
    "        NUMPYimage = np.array(newPIL, dtype='int16')\n",
    "        \n",
    "        ## Preprosessing for background detection --START\n",
    "        ri = api.GetIterator()\n",
    "        level = RIL.WORD\n",
    "        # Word by word iterator \n",
    "        for r in iterate_level(ri, level):\n",
    "            if r:\n",
    "                bbox = r.BoundingBox(level)\n",
    "                #print(bbox)\n",
    "                if bbox: ## Now black (-1) 'em out\n",
    "                    # Word_BBoxes.append(bbox)\n",
    "                    # print(bbox)\n",
    "                    #print(bbox)\n",
    "                    #print(NUMPYimage.shape)\n",
    "                    NUMPYimage[bbox[1]:bbox[3], bbox[0]:bbox[2], :] = -1\n",
    "        ## Preprosessing for background detection --END\n",
    "        \n",
    "        ri = api.GetIterator()\n",
    "        level = RIL.BLOCK # Block based Values \n",
    "\n",
    "        # Table is detectable ( we can get the verticle and horizental Lins )\n",
    "        for r in iterate_level(ri, level):\n",
    "            if r:\n",
    "                #print(\"_+\")\n",
    "                \n",
    "                block_type = r.BlockType() # Type of that specific blocl\n",
    "                tmp = r.GetUTF8Text(level)\n",
    "                \n",
    "                #print(r.ParagraphInfo())\n",
    "                if block_type == PT.TABLE:\n",
    "                    img = r.GetBinaryImage(level)\n",
    "                    img.show()\n",
    "                    #print(\"yes\")\n",
    "                    thisPageText += \"<table>\\n\"\n",
    "                    thisPageText += \"<text>\\n\" + r.GetUTF8Text(level) + \"</text>\\n\"\n",
    "                    thisPageText += \"</table>\\n\"\n",
    "                elif((block_type != PT.UNKNOWN) and (tmp.rstrip() != \"\")):\n",
    "                    ###  Backgrpound Color algo -START Params (bbox, numBIN, NumpyImage)\n",
    "                    rgb_result = line_bg_detection(r.BoundingBox(level), 20, NUMPYimage)\n",
    "                    ###  Backgrpound Color algo -END\n",
    "                    if (rgb_result):\n",
    "                        thisPageText += \"<text font_bg = '\" + rgb_result[0] + \",\" + rgb_result[1] + \",\" + rgb_result[2] +\"'>\\n\" + r.GetUTF8Text(level) + \"</text>\\n\"\n",
    "                    else:\n",
    "                        thisPageText += \"<text>\\n\" + r.GetUTF8Text(level) + \"</text>\\n\"\n",
    "                \n",
    "    FileTextBuffer += thisPageText + \"</page\" +str(pagecounter)+\">\\n\"\n",
    "    pagecounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<document>\n",
      "<page1>\n",
      "<text>\n",
      "— _ We embed the task of identifying bias in the learning phase and iteratively learns from a refined training\n",
      "data whose potential outliers have been re—labeled.\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "—— _ Finally, we conduct several experiments to show the effectiveness of our algorithm on several real datasets.\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "The rest of this paper is organized as follows: section 2 provides a background and related work on kernels. Sections\n",
      "3.1 to 3.4 details the proposed semantic kernel while sections 3.4 — 3.6 outlines techniques incorporated to deal with\n",
      "bias in the data. We provide detailed experimental work and analysis in section 4 and conclude the discussion in this\n",
      "paper in section 5.\n",
      "\n",
      "</text>\n",
      "<text>\n",
      ". Problem setup and background\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "In this section, we define the problem of classification in the presence of label noise. In traditional supervised\n",
      "classification, a learner is first trained on a training set whose labels are known and is then used to classify an unseen\n",
      "testing set about which it has no prior knowledge. The classifier would determine patterns, learn weights, etc. to\n",
      "identify a typical example of a class from the training label and would then attempt to classify the test data to the\n",
      "class whose pattern is exhibited.\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "2.1. Supervised learning\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "Let X be a data matrix having N instances (rows) and p attributes (columns). For the sake of clarity, we will assume\n",
      "the dataset to be a document by word matrix whose rows are documents and whose columns are words and each X;;\n",
      "represents the occurrence of the word j in document i. Each instance vector, denoted as x; = [xil, Xi2» Xigs ***> xip] in\n",
      "a p—dimensional space, has an associated category label y;=k, ke{1l,...,C} where C is the number of possible\n",
      "categories. The vector yy,; contains the categories of all instances. The training data, T, can then be represented as a\n",
      "set of instances x; paired with its label y;, where the training set contains N such examples, given by\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "(1) T = URMi{(xy)}\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "Let n be the set of hypotheses using which the class of instances can be determined. We define hex as a hypothesis\n",
      "that makes a prediction for an instance x; as h(x;) = k, ke{1,...,C} and the learning algorithm then tries to find /\n",
      "that best estimates y. This is done by finding the parameters defining /A ex that would make it equal, or closest, to y.\n",
      "In practice, since we know the correct label y; Vie{1,...,N} for the training data, we can estimate the best hex. One\n",
      "way of doing this is to express the probability distribution over the possible labels given the input test vector x and\n",
      "the training set T, denoted by p(y|x,T,h) where y is the output category of x. It is clear from our notation that the\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "class label is conditional on the input vector x, the training dataset T and the chosen model /. Given a probabilistic\n",
      "output, we can now compute our \"best guess\" to the true label as\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "(2) h(x) = argmax{—,p(y = k|x,T, h)\n",
      "which corresponds to the most probable class label also known as the Maximum Aposteriori (MAP) estimate.\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "The problem of supervised learning can now be expressed in terms of learning the model, /, which best estimates\n",
      "the original label y; in the set of training data or the one which makes the least number of empirical error. The\n",
      "empirical error is defined as the proportion of training instances where the predicted output label, Ah(x;), do not agree\n",
      "with the training label, y;. The empirical error given by the hypothesis / given the training data T is\n",
      "\n",
      "</text>\n",
      "<table>\n",
      "<text>\n",
      "(3) ECRIT) = Xi—, I(h(x;) # y;)\n",
      "The best hypothesis A is therefore one that minimizes equation (3) and is given by\n",
      "(4) h = argmin}q:l ZIiV=1I(hj (x;) £ yi)\n",
      "\n",
      "</text>\n",
      "</table>\n",
      "<text>\n",
      "where I is an indicator function.\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "2.2. Support Vector Machines\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "Support Vector Machines (SVM) was developed by Cortes & Vapnik (Cortes & Vapnik, 1995) for the binary\n",
      "classification task and were first applied to the task of document categorization by (Joachims, 1998). In SVM, we are\n",
      "primarily searching for the optimal separating hyperplane between the two classes. This is done by maximizing the\n",
      "margin between the closest points, 1.e., the points lying closest to the boundary. These points are known as support\n",
      "vectors and are used to find the optimal separating hyperplane as shown in Figure 1. Mathematically speaking, the\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "equation for the separating hyperplane is given by\n",
      "\n",
      "</text>\n",
      "</page1>\n",
      "<page2>\n",
      "<text font_bg = '255,255,255'>\n",
      "A further comparison is made on the two robustness measures, 1.e., RLA and ELA (section 4.2), over the same\n",
      "datasets and noise levels. The results are shown in Table 3. Recall that the RLA shows the relative loss in accuracy\n",
      "with the increase in noise level. Hence, a lower RLA suggests that the algorithm shows a less sharp decrease in\n",
      "accuracy. However, it does not elaborate whether the accuracy was high or low to begin with. The lower the RLA\n",
      "score, the better the algorithm performed. The second measure, the ELA score, shows the deviation from a perfect\n",
      "accuracy of 100% normalized by the accuracy at 0% noise. Like the RLA, a lower value suggests better results. The\n",
      "scores should be looked at in combination with the accuracy value since neither, on its own, reflects how accurate\n",
      "the results are. For instance, for the 20—ORTH dataset, the linear kernel has an RLA value of 0.23 and an ELA value\n",
      "of 0.6. Compared to this, the Polynomial kernel has an RLA value of 0.11 and an ELA score of 0.54. This suggests\n",
      "that the Polynomial kernel not only decreases less sharply (have a comparatively lower RLA score) but also has a\n",
      "better accuracy of the two (lower ELA score). For the same dataset, even though the RBF kernel shows very little\n",
      "decrease in accuracy with the increase in noise, however, the very high ELA score suggests that the accuracy was\n",
      "very low at 0% to begin with. Indeed, it has an accuracy value of 0.51 which is near to a random distribution.\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "When compared to other methods, the CSK has both lower RLA score and a lower ELA score suggesting that it is\n",
      "more robust to noise and also has a comparatively higher accuracy. Consider the most difficult dataset (GENDER)\n",
      "at 25% noise, we observe that CSK has an ELA score of 0.17. No other dataset comes close to this score with the\n",
      "second lowest ELA score being 0.61 for the x—Sim kernel. We must be careful in interpreting these results, however.\n",
      "For instance, the values for the LINGSPAM dataset at 25% noise may suggest that the RBF kernel having an RLA\n",
      "score of 0.00 and an ELA score 0.19 is the best performing method. In reality, however, this is so because the RBF\n",
      "kernels achieves an accuracy of 0.84 at 0% noise which remains the same with the increase in label noise. In terms\n",
      "of accuracy, the CSK method has an accuracy value of 0.97 at 0% noise and comes down to 0.85 at 25% noise. This\n",
      "is still higher than the accuracy achieved by RBF. Hence, for this particular dataset, we may say that RBF kernel 1s\n",
      "more robust but, in general, CSK still has the higher accuracy and a fair level of robustness.\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "<< Table 3 goes here >>\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "Discussion: Overall, the experiments above show the effect of both the CSK and the effect of re—labelling the\n",
      "training data. Interestingly, even in the case where there was no noise, re—labeling was able to significantly boost the\n",
      "test accuracies. This implies that some of the instances in the training labels are capable of greatly influencing the\n",
      "model learnt, even in the presence of the soft margin parameter. It is noteworthy here is that for the 20—ORTH and\n",
      "LINGSPAM datasets, even when the training accuracy slightly decreases (as is the case in 0% noise), the test\n",
      "accuracies increase. This can be attributed to over—fitting the training data but under—fitting the test data (hence lower\n",
      "accuracy value). We attribute this to finding higher—order paths in the CSK where a particular document is labelled\n",
      "with one category but actually contain words that have been attributed to other categories as given by w in equation\n",
      "() and the corresponding score given by equation (). This is further evident since either re—labelling such instances or\n",
      "even removing them from the training data results in better test accuracies. For the former, the test accuracies\n",
      "increase by 8, 3.7, 3.6, and 12.28 percent for the 20—ORTH, 20—REL, LINGSPAM, and GENDER datasets\n",
      "respectively, and for the later by 10, 5.2, 3.8, and 14 percent respectively. Therefore, de—noising the training labels\n",
      "seems to be an important step even in the absence of injected noise labels.\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      ". Conclusion and future work\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "We propose a semantic kernel that uses weighted higher order paths from the given data to compute. Using\n",
      "similarity values from the data matrix is a powerful technique and be used to adjust for bias in the original labels of\n",
      "the training data. Our experimental results show that by using semantic similarities, we are able to improve on the\n",
      "training accuracy in the case of label noise. Introducing the weight parameter o helps to force intra—cluster paths\n",
      "which results in improved test accuracy over the unsupervised (y—Sim) kernel.\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "Secondly, using class re—labelling in the training data can be quite helpful. Outliers in the training set may lead to\n",
      "over—fitting which then deteriorates the test data. This works because the re—labelling is done by the kernel function\n",
      "that computes what the label should be based on weighted higher order paths in the data.\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "The current work has focused on classification on a single data view. An emerging trend, particularly in\n",
      "unsupervised learning, is that of multi—view learning. Multi—view data is collection of data about the same samples\n",
      "but from different sources and represented by different feature set such as movies—by—features and movies—by—actors.\n",
      "It has been shown in the literature, including using the unsupervised (y—Sim) method (Hussain & Bashir, 2016), that\n",
      "multi—view data can help to significantly increase the accuracy of clustering. Both the CSK and the de—noising\n",
      "technique can be used for multi—view data. In particular, the R matrix is generated as a by—product of CSK represent\n",
      "proximities between instances which are the same across different views and can be transferred from one view to the\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "17\n",
      "\n",
      "</text>\n",
      "</page2>\n",
      "<page3>\n",
      "<text>\n",
      "Table 1: Summary of commonly used kernels for sparse, dyadic data\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "Linear faster, good for large, fixed unable to deal with non—linearly separable data, [linearly separable data e.g. text\n",
      "parameters\n",
      "\n",
      "Polynomial somewhat similar to linear where|degree parameter needs to be selected, higher _ |(mostly used in speech\n",
      "\n",
      "-\n",
      "\n",
      "feature similarity\n",
      "\n",
      "RBF has a smoothing parameter to _ |the smoothing parameter can greatly effect data |mostly used in computer\n",
      "determine the influence of each [fitting, no feature—feature similarity vision\n",
      "instance, translation invariant\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "MLP less susceptible to noisy data, _ |only conditionally p.s.d., parameters must be generic\n",
      "has some advantages of neural _ |tuned, no feature—feature similarity\n",
      "networks\n",
      "xy—Sim/ computes feature—feature does not exploit category labels for similarity sparse, high—dimensional data\n",
      "similarity, good for sparse high— (e. g. text, gene expression, link\n",
      "dimensional data analysis)\n",
      "SVM sparsity problem classifier, no feature—feature similarity\n",
      "Kernel\n",
      "Class Meaning |smooths terms by having class— |specific to text, does not take higher—order textual data\n",
      "weighting, low complexity\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "Table 2: Effect of rectifying bias on the training and test micro—F1l scores\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "Data Noise Level\n",
      "O _ J0.998 ____] 0949 ___ | 0.750 ____| 0.810 ____| 8.00 ________\n",
      "20— OKTH\n",
      "O _ J0918 ___ | 0991 ___ | 0.946 ____| 0.981 ____| 3.70 _________|\n",
      "2—RHL\n",
      "O _ J1000 _ |0.986 _ | 0.945 ____| 0.979 ____| 3.60 _________\n",
      "LINGSPAM\n",
      "0 J 0716 ___ | 0864 _ | 0.668 ____| 0.750 ____| 12.28 ________|\n",
      "GENDER\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "24\n",
      "\n",
      "</text>\n",
      "</page3>\n",
      "<page4>\n",
      "<text font_bg = '246,247,249'>\n",
      "Books and Courses\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "Courses | Have Created\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "If you‘re looking for a more in—depth treatment of computer vision, make sure\n",
      "you take a look at the PyImageSearch Gurus course:\n",
      "\n",
      "</text>\n",
      "<text>\n",
      "«4\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "PyimageSearch Gurus\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "13 modules — 168 lessons — 2,161 pages\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "The PyImageSearch Gurus course is similar to a college—level survey course,\n",
      "including 13 modules spanning 168 lessons. We start off with the\n",
      "fundamentals of computer vision and image processing, and then move on to\n",
      "more advanced concepts, including:\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "e Face recognition\n",
      "\n",
      "e Deep learning\n",
      "\n",
      "e Training your own custom object detectors\n",
      "\n",
      "e Automatic License/Number Plate Recognition (ANPR)\n",
      "\n",
      "e Machine learning and image classitfication\n",
      "\n",
      "e Hadoop and big data tools for computer vision\n",
      "\n",
      "e Content—Based Image Retrieval (i.e., image search engines)\n",
      "e ...and more!\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "The PyImageSearch Gurus course is the most in—depth computer vision course\n",
      "available online, so if you‘re serious about studying computer vision, this is the\n",
      "\n",
      "</text>\n",
      "<text font_bg = '255,255,255'>\n",
      "course for you.\n",
      "TELL ME MORE ABOUT THE COURSE\n",
      "\n",
      "</text>\n",
      "<text font_bg = '246,247,249'>\n",
      "Page 11\n",
      "\n",
      "</text>\n",
      "</page4>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(FileTextBuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./11output.xml\", 'w+',encoding=\"utf-8\") as f:\n",
    "    f.write(FileTextBuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
