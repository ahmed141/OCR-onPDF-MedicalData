{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The File for generating HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wand.image import Image as wi\n",
    "import os; from io import BytesIO; import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "from tesserocr import PyTessBaseAPI, RIL, iterate_level, PT, OEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"first app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2folderimages (dir_path, file):\n",
    "    newfolderpath = dir_path + file.split('.')[0]\n",
    "    print(newfolderpath)\n",
    "    try:\n",
    "        new_dir_cmd = \"mkdir \"+ newfolderpath # Creating a new directory for the file \n",
    "        os.system(new_dir_cmd) # Running the OS Command in the file \n",
    "        sys_cmd = \"convert -units PixelsPerInch -density 300 \" + dir_path+file +\" \"+ newfolderpath+\"\\page.png\" # Converting the files \n",
    "        os.system(sys_cmd)\n",
    "        return newfolderpath\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_nicely(l):\n",
    "    \"\"\" Sort the given iterable in the way numerical decimal order rather than binary.\"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page limit \n",
    "* Whether the page is greater than the required threshold limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_page_limit(pdf_file_location, threshold_limit):\n",
    "    \"\"\"\n",
    "    A Function responsible for the page based threshold returning whether to do it or not \n",
    "    \"\"\"\n",
    "    pdf = wi(filename = pdf_file_location, resolution=5 ,background = 'white')\n",
    "    numPages = len(pdf.sequence)\n",
    "    print(numPages) # Debugging\n",
    "    if numPages > threshold_limit:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Page limit on the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.sep.join(['.','datafiles',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\datafiles\\IVA_2016_TEST_CHART_10020_Redacted.pdf\n",
      "9\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_10504_Redacted.pdf\n",
      "58\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_12062_Redacted.pdf\n",
      "95\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_12852_Redacted.pdf\n",
      "72\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_13117_Redacted.pdf\n",
      "7\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_15149_Redacted.pdf\n",
      "20\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_15347_Redacted.pdf\n",
      "18\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_15913_Redacted.pdf\n",
      "5\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_169_Redacted.pdf\n",
      "45\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_18125_Redacted.pdf\n",
      "10\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_18251_Redacted.pdf\n",
      "11\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_18530_Redacted.pdf\n",
      "38\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_18548_Redacted.pdf\n",
      "6\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_19706_Redacted.pdf\n",
      "14\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20143_Redacted.pdf\n",
      "95\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20148_Redacted.pdf\n",
      "4\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20160_Redacted.pdf\n",
      "124\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20338_Redacted.pdf\n",
      "73\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20370_Redacted.pdf\n",
      "34\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20421_Redacted.pdf\n",
      "7\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20590_Redacted.pdf\n",
      "35\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20834_Redacted.pdf\n",
      "50\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20870_Redacted.pdf\n",
      "18\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20916_Redacted.pdf\n",
      "51\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20935_Redacted.pdf\n",
      "15\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20961_Redacted.pdf\n",
      "102\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20965_Redacted.pdf\n",
      "47\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_20978_Redacted.pdf\n",
      "11\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_21074_Redacted.pdf\n",
      "55\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_21240_Redacted.pdf\n",
      "23\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_21291_Redacted.pdf\n",
      "10\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_227_Redacted.pdf\n",
      "71\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_23206_Redacted.pdf\n",
      "27\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_2330_Redacted.pdf\n",
      "29\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_2365_Redacted.pdf\n",
      "5\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_2410_Redacted.pdf\n",
      "42\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_2566_Redacted.pdf\n",
      "71\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_25787_Redacted.pdf\n",
      "18\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_2663_Redacted.pdf\n",
      "14\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_2675_Redacted.pdf\n",
      "25\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_26780_Redacted.pdf\n",
      "83\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_27489_Redacted.pdf\n",
      "69\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_28508_Redacted.pdf\n",
      "30\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_3339_Redacted.pdf\n",
      "44\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_349_Redacted.pdf\n",
      "9\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_41705_Redacted.pdf\n",
      "26\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_41874_Redacted.pdf\n",
      "52\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_42849_Redacted.pdf\n",
      "54\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_46418_Redacted.pdf\n",
      "17\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_49_Redacted.pdf\n",
      "22\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_501_Redacted.pdf\n",
      "29\n",
      "True\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_585_Redacted.pdf\n",
      "55\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_598_Redacted.pdf\n",
      "53\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_68_Redacted.pdf\n",
      "97\n",
      "False\n",
      ".\\datafiles\\IVA_2016_TEST_CHART_9654_Redacted.pdf\n",
      "24\n",
      "True\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_106000_Redacted.pdf\n",
      "56\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_106003_Redacted.pdf\n",
      "111\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_106028_Redacted.pdf\n",
      "75\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_106099_Redacted.pdf\n",
      "13\n",
      "True\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_129970_Redacted.pdf\n",
      "38\n",
      "True\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_129980_Redacted.pdf\n",
      "52\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_130822_Redacted.pdf\n",
      "90\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_143195_Redacted.pdf\n",
      "46\n",
      "True\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_143297_Redacted.pdf\n",
      "3\n",
      "True\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_143462_Redacted.pdf\n",
      "1\n",
      "True\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_153122_Redacted.pdf\n",
      "3\n",
      "True\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_153130_Redacted.pdf\n",
      "129\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_153142_Redacted.pdf\n",
      "88\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_153195_Redacted.pdf\n",
      "120\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_158215_Redacted.pdf\n",
      "8\n",
      "True\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_158220_Redacted.pdf\n",
      "43\n",
      "True\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_158336_Redacted.pdf\n",
      "176\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_168224_Redacted.pdf\n",
      "61\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_168229_Redacted.pdf\n",
      "134\n",
      "False\n",
      ".\\datafiles\\IVA_2017_TEST_CHART_168290_Redacted.pdf\n",
      "27\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "pdf_files = glob.glob(directory+'*.pdf')\n",
    "for pdf_file in pdf_files:\n",
    "    print(pdf_file)\n",
    "    state = pdf_page_limit(pdf_file,50)\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session and Creating of Absolute based image paths\n",
    "* Creating Image path mappings along with direct dataframes creationg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_path = [ [directory +path] for path in list_pdf_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df=spark.createDataFrame(full_image_path,['image_paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         image_paths|\n",
      "+--------------------+\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "|.\\datafiles\\IVA_2...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please map the above configuration in a way\n",
    "\n",
    "pdf_location_path_name, Image_location_path_name, ImageBinary, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_function(data_str):\n",
    "         cleaned_str = 'dummyData'\n",
    "         return data_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"complete_dict.pkl\", 'rb') as f:\n",
    "       dictionary = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytestapi_path = r'C:\\\\Tesseract-OCR\\\\tessdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_dict_counts(tess_word):\n",
    "    '''\n",
    "    params:\n",
    "    string text\n",
    "    returns:\n",
    "    words count\n",
    "    words count, not in dictionary\n",
    "    list of words in input strring\n",
    "    '''\n",
    "    words_count = 0\n",
    "    wordsNotInDict = 0\n",
    "    Words = \"\"\n",
    "    \n",
    "    lower_words = word_tokenizer(tess_word)\n",
    "    for word in lower_words:\n",
    "        words_count += 1\n",
    "        Words += word + \"\\n\"\n",
    "        if word not in dictionary:  #assumption that dictionary is global variuable containing all dictionary words in alist  \n",
    "            wordsNotInDict += 1\n",
    "    \n",
    "    return words_count, wordsNotInDict, Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    tokens = re.findall(\"[a-z]+\", text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dirty Pages UDF Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dirt_from_page(filepath):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    tmpTotal_words = 0\n",
    "    tmpNot_in_dict = 0\n",
    "    tmpWords = \"\"\n",
    "    page = Image.open(filepath) #readimage in PIL format\n",
    "    print(\"MR chart '\", filepath, \"' is under-process\")\n",
    "    DICT_time = 0\n",
    "    \n",
    "    with PyTessBaseAPI(path = pytestapi_path) as api:\n",
    "        api.SetImage(page)\n",
    "        api.Recognize()\n",
    "        ## Preprosessing for background detection --START\n",
    "        ri = api.GetIterator()\n",
    "        level = RIL.WORD\n",
    "        \n",
    "        # Word by word iterator \n",
    "        for r in iterate_level(ri, level):\n",
    "            if r:\n",
    "                s = time.time()\n",
    "                tmp = r.GetUTF8Text(level)\n",
    "                if (tmp.rstrip()):\n",
    "                    w_c, wd_c, w = get_words_dict_counts(r.GetUTF8Text(level).lower())\n",
    "                    tmpTotal_words += w_c\n",
    "                    tmpNot_in_dict += wd_c\n",
    "                    tmpWords += w\n",
    "                DICT_time += time.time() - s\n",
    "                \n",
    "    print(\"Dictionary check time:\\t\", DICT_time)\n",
    "    results['invalidWords'] = tmpNot_in_dict\n",
    "    results['totalWords'] = tmpTotal_words\n",
    "    results['tmpWords'] = tmpWords\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o70.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 76, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-82-a3b56e8cb2ec>\", line 8, in get_dirt_from_page\n  File \"c:\\users\\shafay.amjad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\PIL\\Image.py\", line 2822, in open\n    raise IOError(\"cannot identify image file %r\" % (filename if filename else fp))\nOSError: cannot identify image file '.\\\\datafiles\\\\IVA_2016_TEST_CHART_49_Redacted.pdf'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 76, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-82-a3b56e8cb2ec>\", line 8, in get_dirt_from_page\n  File \"c:\\users\\shafay.amjad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\PIL\\Image.py\", line 2822, in open\n    raise IOError(\"cannot identify image file %r\" % (filename if filename else fp))\nOSError: cannot identify image file '.\\\\datafiles\\\\IVA_2016_TEST_CHART_49_Redacted.pdf'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-31930df120d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdummy_function_udf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_dirt_from_page\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dirt_calculate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy_function_udf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_paths'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o70.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 76, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-82-a3b56e8cb2ec>\", line 8, in get_dirt_from_page\n  File \"c:\\users\\shafay.amjad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\PIL\\Image.py\", line 2822, in open\n    raise IOError(\"cannot identify image file %r\" % (filename if filename else fp))\nOSError: cannot identify image file '.\\\\datafiles\\\\IVA_2016_TEST_CHART_49_Redacted.pdf'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 76, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-82-a3b56e8cb2ec>\", line 8, in get_dirt_from_page\n  File \"c:\\users\\shafay.amjad\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\PIL\\Image.py\", line 2822, in open\n    raise IOError(\"cannot identify image file %r\" % (filename if filename else fp))\nOSError: cannot identify image file '.\\\\datafiles\\\\IVA_2016_TEST_CHART_49_Redacted.pdf'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "dummy_function_udf = udf(get_dirt_from_page, StringType())\n",
    "df = df.withColumn(\"dirt_calculate\", dummy_function_udf(df['image_paths']))\n",
    "df.show()\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Time:\\t\\t\", tmp_time/129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytestapi_path = \"C:\\\\Tesseract\\\\Tesseract-OCR-v5\\\\tessdata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR SCAN\n",
    "* Images to Text Conversion \n",
    "* Table Detection (Table detection does happens in the java code but doesn't go in the grain)\n",
    "     * TH , TR , TD\n",
    "* Background Detection\n",
    "    * Specific Line Background\n",
    "    * Word Level, Block Level & Line Level (Under Progress)\n",
    "    \n",
    "* Font Characterstics\n",
    "    * Font Density ( Tried to do in jar but was inefficent, insufficent and inaccurate)\n",
    "        * Bold \n",
    "        * Light\n",
    "    * Font Size\n",
    "* Hand Written Detection (ITS UNDER PROGRESS)\n",
    "    * HandWritten Model vs Digital Text Model\n",
    "    * Evaluation of Metrices\n",
    "    \n",
    "     \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_bg_detection(bbox, numBIN, NUMPYimage):\n",
    "    if bbox:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        colorbin = np.ones((numBIN, 3), dtype = 'int16') *-1\n",
    "\n",
    "        combs = np.where(NUMPYimage[y1:y2, x1:x2, 0] != -1) # Check if the image didn't had the -1 there\n",
    "        if (combs[0].size != 0): # check if the size is not zero \n",
    "            bin_i = 0\n",
    "            # Choose randomly bins (20 is the default set right now )\n",
    "            # To check the image or the bin or whatsoever \n",
    "            for i in np.random.choice(range(combs[0].shape[0]), numBIN):\n",
    "                x = combs[1][i]\n",
    "                y = combs[0][i]\n",
    "                colorbin[bin_i] = NUMPYimage[y1+y, x1+x, :]\n",
    "                bin_i += 1\n",
    "            # Historgram \n",
    "            hist = {}\n",
    "            for r, g, b in colorbin:\n",
    "                if not (r == -1):\n",
    "                    RGB = str(r)+\"_\"+str(g)+\"_\"+str(b)\n",
    "                    if RGB in list(hist.keys()):\n",
    "                        hist[RGB] += 1\n",
    "                    else:\n",
    "                        hist[RGB] = 1\n",
    "            if len(list(hist.keys())):\n",
    "                return list(hist.keys())[list(hist.values()).index(max(list(hist.values())))].split(\"_\") ## RGB value in list\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_row_tags(api, bbox):\n",
    "    ri = api.GetIterator()\n",
    "    level = RIL.TEXTLINE\n",
    "    ri = api.GetIterator()\n",
    "    \n",
    "    buffer = 0\n",
    "    \n",
    "    lines = \"<table>\\n\"\n",
    "    \n",
    "    for rl in iterate_level(ri, level):\n",
    "        if rl:\n",
    "            tmp = rl.GetUTF8Text(level)\n",
    "            if (tmp.rstrip()):\n",
    "                c_x1, c_y1, c_x2, c_y2  = rl.BoundingBox(level)\n",
    "                if (c_y1 >= (bbox[1] - buffer) and c_y2 <= (bbox[3] + buffer)):\n",
    "                    lines += \"<tr>\" + rl.GetUTF8Text(level) + \"\\n</tr>\\n\"\n",
    "                if (c_y2 > (bbox[3] + buffer)):\n",
    "                    break\n",
    "    lines += \"</table>\\n\"\n",
    "    # print(lines)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_nicely(l):\n",
    "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_HTML (FOLDERpath):\n",
    "    '''\n",
    "    params:\n",
    "    FOLDERpath --> path of directoray which contain all the MR document images\n",
    "    returns:\n",
    "    HTML format string containing result of OCR scan    \n",
    "    '''\n",
    "    \n",
    "    FileTextBuffer = \"\"\n",
    "    FileTextBuffer += \"<document>\\n\"\n",
    "\n",
    "    filesList = os.listdir(FOLDERpath)\n",
    "    \n",
    "    pagecounter  = 1\n",
    "    \n",
    "    WordsofPages = FileTextBuffer\n",
    "    for filename in sorted_nicely(filesList):\n",
    "        WordsofPages += \"<page\"+str(pagecounter)+\">\\n\"\n",
    "        \n",
    "        start = time.time()\n",
    "        page = Image.open(FOLDERpath+\"\\\\\"+filename) #readimage in PIL format\n",
    "        print(\"MR chart '\", FOLDERpath+\"\\\\\"+filename, \"' is under-process\")\n",
    "        \n",
    "        thisPageText = \"\"\n",
    "        thisPageText += \"<page\" +str(pagecounter)+\">\\n\"\n",
    "\n",
    "        with PyTessBaseAPI(path = pytestapi_path) as api:\n",
    "            api.SetImage(page)\n",
    "            api.Recognize()\n",
    "            \n",
    "            if (page.mode != 'RGB'): #check if image not in RGB then make it one: for bg color detection\n",
    "                NUMPYimage = np.array(page.convert('RGB'), dtype='int16')\n",
    "            else:\n",
    "                NUMPYimage = np.array(page, dtype='int16')\n",
    "\n",
    "            ## Preprosessing for background detection --START\n",
    "            ri = api.GetIterator()\n",
    "            level = RIL.WORD\n",
    "            # Word by word iterator \n",
    "            for r in iterate_level(ri, level):\n",
    "                if r:\n",
    "                    bbox = r.BoundingBox(level)\n",
    "                    WordsofPages += r.GetUTF8Text(level).lower() + \"\\n\"\n",
    "                    if bbox: ## Now black (-1) 'em out\n",
    "                        NUMPYimage[bbox[1]:bbox[3], bbox[0]:bbox[2], :] = -1\n",
    "            ## Preprosessing for background detection --END\n",
    "\n",
    "            ri = api.GetIterator()\n",
    "            level = RIL.BLOCK # Block based Values \n",
    "\n",
    "            # Table is detectable ( we can get the verticle and horizental Lins )\n",
    "            for r in iterate_level(ri, level):\n",
    "                if r:\n",
    "                    block_type = r.BlockType() # Type of that specific block\n",
    "                    # print(r.GetUTF8Text(level))\n",
    "                    \n",
    "                    if (block_type == PT.TABLE):\n",
    "                        img = r.GetBinaryImage(level)\n",
    "                        # img.show() to show cropped table image\n",
    "                        text_rows = get_table_row_tags(api, r.BoundingBox(level))\n",
    "                        thisPageText += text_rows\n",
    "                    elif(block_type not in [PT.UNKNOWN, PT.FLOWING_IMAGE, PT.HEADING_IMAGE, PT.PULLOUT_IMAGE, PT.HORZ_LINE, PT.VERT_LINE, PT.NOISE]):\n",
    "                        ###  Backgrpound Color algo -START Params (bbox, numBIN, NumpyImage)\n",
    "                        rgb_result = line_bg_detection(r.BoundingBox(level), 20, NUMPYimage)\n",
    "                        ###  Backgrpound Color algo -END\n",
    "                        if (rgb_result):\n",
    "                            thisPageText += \"<text font_bg = '\" + rgb_result[0] + \",\" + rgb_result[1] + \",\" + rgb_result[2] +\"'>\\n\" + r.GetUTF8Text(level) + \"</text>\\n\"\n",
    "                        else:\n",
    "                            thisPageText += \"<text>\\n\" + r.GetUTF8Text(level) + \"</text>\\n\"\n",
    "\n",
    "        FileTextBuffer += thisPageText + \"</page\" +str(pagecounter)+\">\\n\"\n",
    "        WordsofPages += \"<\\page\"+str(pagecounter)+\">\\n\"\n",
    "        \n",
    "        pagecounter += 1\n",
    "        print(\"Page Time:\\t\\t\", time.time() - start)\n",
    "        \n",
    "    FileTextBuffer += \"</document>\\n\"\n",
    "    WordsofPages += \"</document>\\n\"\n",
    "    \n",
    "    return FileTextBuffer, WordsofPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDERpath = '..\\\\datafiles\\\\doc22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COST = time.time()\n",
    "GeneratedText, GeneratedWords = generate_HTML(FOLDERpath)\n",
    "tmp_time = time.time() - COST\n",
    "print(\"Time on Document:\\t\", tmp_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average time:\\t\\t\", tmp_time/52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(GeneratedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc22-text8.xml', 'w+') as f:\n",
    "    f.write(GeneratedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc22-words-2.xml', 'w+') as f:\n",
    "    f.write(GeneratedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
